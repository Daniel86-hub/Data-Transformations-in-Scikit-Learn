{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d967b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c69af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer=load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6897f282",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "(143, 30)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train, y_test=train_test_split(cancer.data, cancer.target, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9637a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's import the class that implements the preprocessing and instantiate it\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "\n",
    "#Lets fit the scaler\n",
    "\n",
    "scaler.fit(X_train)          #The fit method computes the min and max values of each feature in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68512d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformtion that we just learned(scaling the training data)\n",
    "# transform data\n",
    "\n",
    "X_train_scaled=scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f2fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed shape: (426, 30)\n",
      "per-feature minimum before scaling:\n",
      " [6.981e+00 9.710e+00 4.379e+01 1.435e+02 5.263e-02 1.938e-02 0.000e+00\n",
      " 0.000e+00 1.060e-01 5.024e-02 1.153e-01 3.602e-01 7.570e-01 6.802e+00\n",
      " 1.713e-03 2.252e-03 0.000e+00 0.000e+00 9.539e-03 8.948e-04 7.930e+00\n",
      " 1.202e+01 5.041e+01 1.852e+02 7.117e-02 2.729e-02 0.000e+00 0.000e+00\n",
      " 1.566e-01 5.521e-02]\n",
      "per-feature maximum before scaling:\n",
      " [2.811e+01 3.928e+01 1.885e+02 2.501e+03 1.634e-01 2.867e-01 4.268e-01\n",
      " 2.012e-01 3.040e-01 9.575e-02 2.873e+00 4.885e+00 2.198e+01 5.422e+02\n",
      " 3.113e-02 1.354e-01 3.960e-01 5.279e-02 6.146e-02 2.984e-02 3.604e+01\n",
      " 4.954e+01 2.512e+02 4.254e+03 2.226e-01 9.379e-01 1.170e+00 2.910e-01\n",
      " 5.774e-01 1.486e-01]\n",
      "per-feature minimum after scaling:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "per-feature maximum after scaling:\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Print dataset properties before and after scaling\n",
    "\n",
    "print(\"transformed shape: {}\".format(X_train_scaled.shape))\n",
    "print(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\n",
    "print(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\n",
    "print(\"per-feature minimum after scaling:\\n {}\".format(X_train_scaled.min(axis=0)))\n",
    "print(\"per-feature maximum after scaling:\\n {}\".format(X_train_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cea2727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test_scaled shape:\n",
      " (143, 30)\n",
      "per-feature minimum after scaling:\n",
      " [ 0.0336031   0.0226581   0.03144219  0.01141039  0.14128374  0.04406704\n",
      "  0.          0.          0.1540404  -0.00615249 -0.00137796  0.00594501\n",
      "  0.00430665  0.00079567  0.03919502  0.0112206   0.          0.\n",
      " -0.03191387  0.00664013  0.02660975  0.05810235  0.02031974  0.00943767\n",
      "  0.1094235   0.02637792  0.          0.         -0.00023764 -0.00182032]\n",
      "per-feature maximum after scaling:\n",
      " [0.9578778  0.81501522 0.95577362 0.89353128 0.81132075 1.21958701\n",
      " 0.87956888 0.9333996  0.93232323 1.0371347  0.42669616 0.49765736\n",
      " 0.44117231 0.28371044 0.48703131 0.73863671 0.76717172 0.62928585\n",
      " 1.33685792 0.39057253 0.89612238 0.79317697 0.84859804 0.74488793\n",
      " 0.9154725  1.13188961 1.07008547 0.92371134 1.20532319 1.63068851]\n"
     ]
    }
   ],
   "source": [
    "# transform test data\n",
    "\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "# print test data properties after scaling\n",
    "print(\"x_test_scaled shape:\\n {}\".format(X_test_scaled.shape))\n",
    "print(\"per-feature minimum after scaling:\\n {}\".format(X_test_scaled.min(axis=0)))\n",
    "print(\"per-feature maximum after scaling:\\n {}\".format(X_test_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645406e8",
   "metadata": {},
   "source": [
    "# Scaling Training and Test data the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3a39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# make syntetic data\n",
    "\n",
    "X,_=make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n",
    "\n",
    "# split data into training and test sets\n",
    "\n",
    "X_train, X_test=train_test_split(X, random_state=5, test_size=.1)\n",
    "\n",
    "#plot the training and test sets\n",
    "\n",
    "#fig, axes=plt.subplots(1, 3, figsize=(13, 4))\n",
    "#axes[0].scatter(X_train[:,0], X_train[:,1], c=mglearn.cm2(0), label=\"Training ste\", s=60)\n",
    "#axes[0].scatter(X_test[:,0], X_test[:,1], marker=\"^\", c=\"mglearn.cm2(1)\", label=\"Test ste\", s=60)\n",
    "\n",
    "#axes[0].legend(loc=\"upper left\")\n",
    "#axes[0].set_title(\"Original data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d87265",
   "metadata": {},
   "source": [
    "# Shortcuts and efficient alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92ace6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "x_scaled=scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f27fbea",
   "metadata": {},
   "source": [
    "While fit_transform is not necessarily more efficient for all models, it is still **good practice** to use this method when trying to transform the training data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b1d11",
   "metadata": {},
   "source": [
    "# Effect of Preprocessing on Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a35849db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test,y_train, y_test=train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "svm=SVC(C=100)\n",
    "svm.fit(X_train, y_train)\n",
    "print(\"Test set score: {:.2f}\".format(svm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020e39b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled test set accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Now, let's scale the data before fitting the SVC\n",
    "\n",
    "#Preprocessing using 0-1 scaling\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "x_train_scaled=scaler.transform(X_train)\n",
    "x_test_scaled=scaler.transform(X_test)\n",
    "\n",
    "# learning an SVM on scaled training data\n",
    "svm.fit(x_train_scaled,y_train)\n",
    "\n",
    "# scoring on test data set\n",
    "\n",
    "print(\"scaled test set accuracy: {:.2f}\".format(svm.score(x_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f08430",
   "metadata": {},
   "source": [
    "###    Preprocessing using zero mean and unit variance scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa02b382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm test set accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "x_train_scaled=scaler.transform(X_train)\n",
    "x_test_scaled=scaler.transform(X_test)\n",
    "\n",
    "# learning an svm on the scaled training data\n",
    "svm.fit(x_train_scaled, y_train)\n",
    "\n",
    "# scoring on test data set\n",
    "\n",
    "print(\"svm test set accuracy: {:.2f}\".format(svm.score(x_test_scaled, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
